import streamlit as st
import tempfile
import os
from pypdf import PdfReader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain_community.vectorstores.faiss import FAISS
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.documents import Document
import warnings
warnings.filterwarnings("ignore")

st.set_page_config(
    page_title="PDF AI Chatbot",
    page_icon="ğŸ“š",
    layout="wide"
)

st.title("ğŸ“š PDF AI Chatbot")
st.markdown("PDF ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”!")

# ì‚¬ì´ë“œë°”ì— ì„¤ì • ì˜µì…˜ë“¤
with st.sidebar:
    # ì‚¬ìš©ë²• ë¨¼ì € í‘œì‹œ
    with st.expander("â„¹ï¸ ì‚¬ìš©ë²•"):
        st.markdown("""
        1. ì‚¬ì´ë“œë°”ì—ì„œ **ëª¨ë¸**ê³¼ **ë¶„ì„ ëª¨ë“œ**ë¥¼ ì„ íƒí•˜ì„¸ìš”
        2. **PDF íŒŒì¼**ì„ ì—…ë¡œë“œí•˜ì„¸ìš”
        3. íŒŒì¼ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ë©´ ì˜¤ë¥¸ìª½ ì±„íŒ…ì°½ì—ì„œ **ì§ˆë¬¸**í•˜ì„¸ìš”
        4. AIê°€ PDF ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•´ë“œë¦½ë‹ˆë‹¤
        
        **ğŸ”§ ë¶„ì„ ëª¨ë“œ ê°€ì´ë“œ:**
        - âš¡ **ë¹ ë¥¸ ë¶„ì„**: ì†ë„ ìš°ì„ , ì¼ë°˜ì ì¸ ì§ˆë¬¸ì— ì í•©
        - ğŸ” **ì •ë°€í•œ ë¶„ì„**: ì •í™•ë„ ìš°ì„ , ì„¸ë¶€ì ì¸ ë‚´ìš© ë¶„ì„ ì‹œ ì‚¬ìš©
        - ğŸ¯ **ê· í˜•ì¡íŒ ë¶„ì„**: ì†ë„ì™€ ì •í™•ë„ì˜ ê· í˜• (ê¶Œì¥)
        
        **ì§€ì› íŒŒì¼:** PDF | **ê¸°ìˆ  ìŠ¤íƒ:** Ollama, LangChain, FAISS
        """)
    
    st.header("âš™ï¸ ì„¤ì •")
    
    # Ollama ëª¨ë¸ ì„ íƒ
    model_name = st.selectbox(
        "ì‚¬ìš©í•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”:",
        ["qwen3:8b", "llama3.2", "mistral"]
    )
    
    # ë¶„ì„ ì •ë°€ë„ ì„¤ì •
    analysis_mode = st.selectbox(
        "ë¶„ì„ ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”:",
        ["âš¡ ë¹ ë¥¸ ë¶„ì„ (ê¶Œì¥)", "ğŸ” ì •ë°€í•œ ë¶„ì„", "ğŸ¯ ê· í˜•ì¡íŒ ë¶„ì„"]
    )
    
    # ë¶„ì„ ëª¨ë“œì— ë”°ë¥¸ íŒŒë¼ë¯¸í„° ë§¤í•‘
    if analysis_mode == "âš¡ ë¹ ë¥¸ ë¶„ì„ (ê¶Œì¥)":
        chunk_size, chunk_overlap = 1500, 75  # í° ì²­í¬, ì ì€ ì˜¤ë²„ë©
    elif analysis_mode == "ğŸ” ì •ë°€í•œ ë¶„ì„":
        chunk_size, chunk_overlap = 800, 150   # ì‘ì€ ì²­í¬, ë§ì€ ì˜¤ë²„ë©
    else:  # ê· í˜•ì¡íŒ ë¶„ì„
        chunk_size, chunk_overlap = 1000, 100  # ê¸°ë³¸ê°’
    
    st.markdown("---")
    
    # íŒŒì¼ ì—…ë¡œë“œ
    uploaded_file = st.file_uploader(
        "PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
        type="pdf",
        accept_multiple_files=False
    )

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = None

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if "processed_file" not in st.session_state:
    st.session_state.processed_file = None

def extract_text_from_pdf(pdf_file):
    """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    text = ""
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(pdf_file.getvalue())
            tmp_file.flush()
            
            reader = PdfReader(tmp_file.name)
            for page in reader.pages:
                text += page.extract_text()
        
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        os.unlink(tmp_file.name)
        
    except Exception as e:
        st.error(f"PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        return ""
    
    return text

def create_vectorstore(text, model_name, chunk_size, chunk_overlap):
    """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ê³  ë²¡í„° ìŠ¤í† ì–´ ìƒì„±"""
    if not text.strip():
        st.error("ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    try:
        # Document ê°ì²´ ìƒì„±
        document = Document(page_content=text, metadata={"source": "uploaded_pdf"})
        
        # í…ìŠ¤íŠ¸ ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
        )
        
        chunks = text_splitter.split_documents([document])
        
        if not chunks:
            st.error("í…ìŠ¤íŠ¸ ì²­í¬ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return None
        
        # Ollama ì„ë² ë”© ìƒì„± (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
        max_retries = 3
        for attempt in range(max_retries):
            try:
                embeddings = OllamaEmbeddings(
                    model="embeddinggemma",
                    base_url="http://localhost:11434"
                )
                
                # FAISS ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
                vectorstore = FAISS.from_documents(chunks, embeddings)
                return vectorstore
                
            except Exception as retry_error:
                if attempt == max_retries - 1:
                    raise retry_error
                st.warning(f"ì„ë² ë”© ìƒì„± ì¬ì‹œë„ ì¤‘... ({attempt + 1}/{max_retries})")
                import time
                time.sleep(2)
        
        return None
        
    except Exception as e:
        st.error(f"ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        st.error("Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return None

def format_docs(docs):
    """ë¬¸ì„œë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í¬ë§·íŒ…"""
    return "\n\n".join(doc.page_content for doc in docs)

def get_conversation_chain(vectorstore, model_name):
    """ëŒ€í™” ì²´ì¸ ìƒì„±"""
    try:
        llm = ChatOllama(
            model=model_name, 
            temperature=0.7,
            base_url="http://localhost:11434"
        )
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
        prompt = PromptTemplate(
            input_variables=["context", "question"],
            template="""
            ë‹¤ìŒ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:

            ë¬¸ì„œ:
            {context}

            ì§ˆë¬¸: {question}
            
            ë‹µë³€ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ í•´ì£¼ì‹œê³ , ë¬¸ì„œì— ìˆëŠ” ì •ë³´ë§Œì„ ì‚¬ìš©í•´ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”.
            ë§Œì•½ ë¬¸ì„œì— ê´€ë ¨ ì •ë³´ê°€ ì—†ë‹¤ë©´ "ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•´ì£¼ì„¸ìš”.
            
            ë‹µë³€:
            """
        )
        
        # LCEL ì²´ì¸ ìƒì„± (StuffDocumentsChain ëŒ€ì²´)
        rag_chain = (
            {"context": vectorstore.as_retriever() | format_docs, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )
        
        return rag_chain
        
    except Exception as e:
        st.error(f"ëŒ€í™” ì²´ì¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

# ğŸ“„ ë¬¸ì„œ ì²˜ë¦¬ ì˜ì—­ (ìƒë‹¨)
st.header("ğŸ“„ ë¬¸ì„œ ì²˜ë¦¬")

if uploaded_file is not None:
    # íŒŒì¼ì´ ë³€ê²½ë˜ì—ˆê±°ë‚˜ ì²˜ìŒ ì—…ë¡œë“œëœ ê²½ìš°ì—ë§Œ ì²˜ë¦¬
    if st.session_state.processed_file != uploaded_file.name:
        with st.spinner("PDF ì²˜ë¦¬ ì¤‘..."):
            # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text = extract_text_from_pdf(uploaded_file)
            
            if text:
                st.success("PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ!")
                st.info(f"ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)} ë¬¸ì")
                
                # í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°
                with st.expander("ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°"):
                    st.text(text[:500] + "..." if len(text) > 500 else text)
                
                # ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
                with st.spinner("ë²¡í„° ì„ë² ë”© ìƒì„± ì¤‘..."):
                    vectorstore = create_vectorstore(text, model_name, chunk_size, chunk_overlap)
                    
                    if vectorstore:
                        st.session_state.vectorstore = vectorstore
                        st.session_state.processed_file = uploaded_file.name
                        st.success("ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ! ì´ì œ ì•„ë˜ ì±„íŒ…ì°½ì—ì„œ ì§ˆë¬¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    else:
                        st.error("ë²¡í„° ìŠ¤í† ì–´ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            else:
                st.error("PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        # ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ì¸ ê²½ìš°
        col1, col2 = st.columns([3, 1])
        with col1:
            st.success(f"ğŸ“ **{uploaded_file.name}** íŒŒì¼ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
        with col2:
            if st.button("ğŸ”„ ë‹¤ë¥¸ íŒŒì¼ ì—…ë¡œë“œ"):
                st.session_state.vectorstore = None
                st.session_state.processed_file = None
                st.session_state.messages = []
                st.rerun()
else:
    st.info("ğŸ‘† ì‚¬ì´ë“œë°”ì—ì„œ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")

st.divider()  # êµ¬ë¶„ì„  ì¶”ê°€

# ğŸ’¬ AI ì±„íŒ… ì˜ì—­ (í•˜ë‹¨)
st.header("ğŸ’¬ AI ì±—ë´‡")

# ì±„íŒ… ì»¨í…Œì´ë„ˆ ìƒì„±
chat_container = st.container()

# ì‚¬ìš©ì ì…ë ¥
if prompt := st.chat_input("PDF ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”..."):
    if st.session_state.vectorstore is None:
        st.error("ë¨¼ì € PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    else:
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # AI ì‘ë‹µ ìƒì„±
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
            try:
                conversation = get_conversation_chain(st.session_state.vectorstore, model_name)
                
                if conversation:
                    answer = conversation.invoke(prompt)
                    st.session_state.messages.append({"role": "assistant", "content": answer})
                else:
                    error_msg = "ëŒ€í™” ì²´ì¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                    st.session_state.messages.append({"role": "assistant", "content": error_msg})
                    
            except Exception as e:
                error_msg = f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                st.session_state.messages.append({"role": "assistant", "content": error_msg})
        
        # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨
        st.rerun()
    
# ì±„íŒ… ê¸°ë¡ì„ ì—­ìˆœìœ¼ë¡œ í‘œì‹œ (ìµœì‹  ë©”ì‹œì§€ê°€ ìœ„ë¡œ)
with chat_container:
    for message in reversed(st.session_state.messages):
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

